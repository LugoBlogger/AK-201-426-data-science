{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs   \n",
    "- [2023/03/08]   \n",
    "  Restart this notebook if you change the scratch library\n",
    "\n",
    "- [2024/04/15]   \n",
    "  You do not need to restart this notebook when updating the scratch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "from typing import List, Callable, Iterable, Tuple\n",
    "from scratch.neural_networks import NeuralNetworks as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a technique that utilize neural network with many layers\n",
    "to solve many problems including supervise and unsupervise.\n",
    "\n",
    "To do deep learning, we need several abstraction of data structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensor\n",
    "\n",
    "The implementation of Tensor type in here using some kind of cheating using List.  \n",
    "We do this because for the practical purpose to learn the concept first.   \n",
    "More general Tensor datatype is provided by popular library like TensorFlow or PyTorch\n",
    "\n",
    "Here we use Tensor data type as a list (in fact in concise\n",
    "mathematical term, $n$-dimensional array *is not*\n",
    "a tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find a tensors' shape:\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "  sizes: List[int] = []\n",
    "  while isinstance(tensor, list):\n",
    "    sizes.append(len(tensor))\n",
    "    tensor = tensor[0]    # we enter the first element and recursively fo the deeper elements\n",
    "  return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n",
    "assert shape([[[1, 2], [2, 3], [4, 5]],\n",
    "       [[6, 7], [8, 9], [10, 11]]]) == [2, 3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement 1d case tensor with its function modification of tensor    \n",
    "and generalization greater than 1d can be implemented using recursive function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "  \"\"\"If tensor[0] is a list, it's a higher-order tensor.\n",
    "     Otherwise, tensor is 1-dimensional (that is, a vector).\"\"\" \n",
    "  return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive `tensor_sum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "  \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "  if is_1d(tensor):\n",
    "    return sum(tensor)      # just a list of floats, use Python sum\n",
    "  else:\n",
    "    return sum(tensor_sum(tensor_i)     # Call tensor_sum on each row\n",
    "                for tensor_i in tensor) # and sum up those results.\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive function to apply a function to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "  \"\"\"Applies f elementwise\"\"\"\n",
    "  if is_1d(tensor):\n",
    "    return [f(x) for x in tensor]\n",
    "  else:\n",
    "    return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2*x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above `tensor_apply` to create a zero tensor with the same shape as   \n",
    "a given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "  return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operation of two tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                    t1: Tensor, t2: Tensor) -> Tensor:\n",
    "  \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "  if is_1d(t1):\n",
    "    return [f(x, y) for x, y in zip(t1, t2)]\n",
    "  else:\n",
    "    return [tensor_combin(f, t1_i, t2_i) for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Layer` class will define an abstraction to derive a specifi layer.    \n",
    "A layer is a function that perform multidimensional array operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \"\"\"Our neural networks will composed of Layers, each of which knows how to do  \n",
    "  some computation on its inputs in the \"forward\" direction and propagate\n",
    "  gradients in the \"backward\" direction\"\"\"\n",
    "\n",
    "  def forward(self, input):\n",
    "    \"\"\"Note the lack of types. We're not going to be presriptive about what kinds\n",
    "    of input layers can take and what kinds of outputs they can return\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def backward(self, gradient):\n",
    "    \"\"\"Similarly, we're not going to be prescriptive about what the gradient \n",
    "    looks like. It's up to you the user to make sure that you're doing things\n",
    "    sensibly\"\"\"\n",
    "\n",
    "  def params(self) -> Iterable[Tensor]:\n",
    "    \"\"\"Returns the parameters of this layer. The default implementation return\n",
    "    nothing, so that if you have a layer with no parameters you don't have to \n",
    "    implement this.\"\"\"\n",
    "    return ()\n",
    "\n",
    "  def grads(self) -> Iterable[Tensor]:\n",
    "    \"\"\"Returns the gradients, in the same order as params()\"\"\"\n",
    "    return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above layer class is an abstraction of specific layer that will be defined   \n",
    "by inherit from that class. In here we called `Layer` class above as a parent  \n",
    "class, and all the specific class can be derived from the parent class.\n",
    "\n",
    "In each specific layer, we can update parameters (`params` variables) in our  \n",
    "networks using its gradient. We can also get from each specific layer its  \n",
    "parameters and gradients.\n",
    "\n",
    "Let us define a specific class `Sigmoid` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3133579176.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid(Layer):\n",
    "  def forward(self, input: Tensor) -> Tensor:\n",
    "    \"\"\"Apply sigmoid to each element of the input tensor, and save the results \n",
    "    to use in backpropagation.\"\"\"\n",
    "    self.sigmoid = tensor_apply(sigmoid, input)\n",
    "    return self.sigmoids\n",
    "\n",
    "  def backwards(self, gradient: Tensor) -> Tensor:\n",
    "    return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                          self.sigmoids, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks as a Sequence of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: XOR Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: FizzBuzz Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmaxes and Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
