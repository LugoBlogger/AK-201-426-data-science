{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs   \n",
    "- [2023/03/08]   \n",
    "  Restart this notebook if you change the scratch library\n",
    "\n",
    "- [2024/04/15]   \n",
    "  You do not need to restart this notebook when updating the scratch library\n",
    "\n",
    "Notes:\n",
    "- You should not use the following implementation of tensor and all the other   \n",
    "  layer abstraction to the real problem. Because they are very slow. Use   \n",
    "  existing library such as TensorFlow or Pytorch for fast running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from typing import List, Callable, Iterable, Tuple\n",
    "from scratch.linear_algebra import LinearAlgebra as la\n",
    "from scratch.probability import Probability as prob\n",
    "from scratch.neural_networks import NeuralNetworks as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a technique that utilize neural network with many layers\n",
    "to solve many problems including supervise and unsupervise.\n",
    "\n",
    "To do deep learning, we need several abstraction of data structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensor\n",
    "\n",
    "The implementation of Tensor type in here using some kind of cheating using List.  \n",
    "We do this because for the practical purpose to learn the concept first.   \n",
    "More general Tensor datatype is provided by popular library like TensorFlow or PyTorch\n",
    "\n",
    "Here we use Tensor data type as a list (in fact in concise\n",
    "mathematical term, $n$-dimensional array *is not*\n",
    "a tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find a tensors' shape:\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "  sizes: List[int] = []\n",
    "  while isinstance(tensor, list):\n",
    "    sizes.append(len(tensor))\n",
    "    tensor = tensor[0]    # we enter the first element and recursively fo the deeper elements\n",
    "  return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n",
    "assert shape([[[1, 2], [2, 3], [4, 5]],\n",
    "       [[6, 7], [8, 9], [10, 11]]]) == [2, 3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement 1d case tensor with its function modification of tensor    \n",
    "and generalization greater than 1d can be implemented using recursive function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "  \"\"\"If tensor[0] is a list, it's a higher-order tensor.\n",
    "     Otherwise, tensor is 1-dimensional (that is, a vector).\"\"\" \n",
    "  return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive `tensor_sum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "  \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "  if is_1d(tensor):\n",
    "    return sum(tensor)      # just a list of floats, use Python sum\n",
    "  else:\n",
    "    return sum(tensor_sum(tensor_i)     # Call tensor_sum on each row\n",
    "                for tensor_i in tensor) # and sum up those results.\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive function to apply a function to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "  \"\"\"Applies f elementwise\"\"\"\n",
    "  if is_1d(tensor):\n",
    "    return [f(x) for x in tensor]\n",
    "  else:\n",
    "    return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2*x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above `tensor_apply` to create a zero tensor with the same shape as   \n",
    "a given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "  return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise operation of two tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                    t1: Tensor, t2: Tensor) -> Tensor:\n",
    "  \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "  if is_1d(t1):\n",
    "    return [f(x, y) for x, y in zip(t1, t2)]\n",
    "  else:\n",
    "    return [tensor_combin(f, t1_i, t2_i) for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Layer` class will define an abstraction to derive a specifi layer.    \n",
    "A layer is a function that perform multidimensional array operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  \"\"\"Our neural networks will composed of Layers, each of which knows how to do  \n",
    "  some computation on its inputs in the \"forward\" direction and propagate\n",
    "  gradients in the \"backward\" direction\"\"\"\n",
    "\n",
    "  def forward(self, input):\n",
    "    \"\"\"Note the lack of types. We're not going to be presriptive about what kinds\n",
    "    of input layers can take and what kinds of outputs they can return\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def backward(self, gradient):\n",
    "    \"\"\"Similarly, we're not going to be prescriptive about what the gradient \n",
    "    looks like. It's up to you the user to make sure that you're doing things\n",
    "    sensibly\"\"\"\n",
    "\n",
    "  def params(self) -> Iterable[Tensor]:\n",
    "    \"\"\"Returns the parameters of this layer. The default implementation return\n",
    "    nothing, so that if you have a layer with no parameters you don't have to \n",
    "    implement this.\"\"\"\n",
    "    return ()\n",
    "\n",
    "  def grads(self) -> Iterable[Tensor]:\n",
    "    \"\"\"Returns the gradients, in the same order as params()\"\"\"\n",
    "    return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above layer class is an abstraction of specific layer that will be defined   \n",
    "by inherit from that class. In here we called `Layer` class above as a parent  \n",
    "class, and all the specific class can be derived from the parent class.\n",
    "\n",
    "In each specific layer, we can update parameters (`params` variables) in our  \n",
    "networks using its gradient. We can also get from each specific layer its  \n",
    "parameters and gradients.\n",
    "\n",
    "Let us define a specific class `Sigmoid` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "  def forward(self, input: Tensor) -> Tensor:\n",
    "    \"\"\"Apply sigmoid to each element of the input tensor, and save the results \n",
    "    to use in backpropagation.\"\"\"\n",
    "    self.sigmoid = tensor_apply(sigmoid, input)\n",
    "    return self.sigmoids\n",
    "\n",
    "  def backwards(self, gradient: Tensor) -> Tensor:\n",
    "    return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                          self.sigmoids, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the implementation of `backward` for sigmoid function, see  \n",
    "[`ch-18-neural-networks.ipynb`](./ch-18-neural-networks.ipynb) or\n",
    "[`neural-nets.drawio`](./img-resources/neural-nets.drawio). We can say in  \n",
    "general that `sig * (1 - sig)` is the derivative of sigmoid to its argument   \n",
    "by representing the result with `sigmoid` function, and `grad` is   \n",
    "the gradient propagation from the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer implements linear layer from Chapter 18 which is a linear function\n",
    "that is defined by \n",
    "$$\n",
    "  \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "The bias term $b$ can be incorporated to the weights vector $\\mathbf{w}$ by \n",
    "concatenating 1 to inputs vector $\\mathbf{x}$. Therefore we we have\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "    w_1 & w_2 & \\ldots & w_n & w_{n+1}\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three options to initialize weights vector:\n",
    "- random uniform distribution on $[0, 1]$\n",
    "- standard normal distribution\n",
    "- Xavier initialization, each weight is initialized with a random draw from  \n",
    "  a normal distribution with mean 0 and variance `2 / (num_inputs + num_outputs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_uniform(*dims: int, rng=np.random.default_rng()) -> Tensor:\n",
    "  if len(dims) == 1:\n",
    "    return [rng.random() for _ in range(dims[0])]\n",
    "  else:\n",
    "    return [random_uniform(*dims[1:], rng=rng) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int, mean: float = 0.0, variance: float = 1.0,\n",
    "                  rng=np.random.default_rng()) -> Tensor:\n",
    "  if len(dims) == 1:\n",
    "    return [mean + variance * prob.inverse_normal_cdf(rng.random())\n",
    "            for _ in range(dims[0])]\n",
    "  else:\n",
    "    return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "            for _ in range(dims[0])]\n",
    "\n",
    "seed = 24_04_26\n",
    "rng = np.random.default_rng(seed)\n",
    "# random_uniform(2, 3, 4, rng=rng)\n",
    "assert shape(random_uniform(2, 3, 4, rng=rng)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10, rng=rng)) == [5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the random generator above in a `random_tensor` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal', \n",
    "                  rng=np.random.default_rng()) -> Tensor:\n",
    "  if init == 'normal':\n",
    "    return random_normal(*dims, rng=rng)\n",
    "  elif init == 'uniform':\n",
    "    return random_uniform(*dims, rng=rng)\n",
    "  elif init == 'xavier':\n",
    "    variance = len(dims) / sum(dims)\n",
    "    return random_normal(*dims, variance=variance)\n",
    "  else:\n",
    "    raise ValueError(f\"unknown init: {init}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a linear layer we need the following integers to defined\n",
    "- the dimension of the inputs (which tells us how many weights each neuron neds) \n",
    "- the dimension of the outputs (which tell us how many neurons we should have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier',\n",
    "                rng=np.random.default_rng()) -> None:\n",
    "    \"\"\"A layer of output_dim neurons, each with input_dim weights (and a bias).\"\"\"\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    \n",
    "    # self.w[o] is the weights for the o-th neuron\n",
    "    self.w = random_tensor(output_dim, input_dim, init=init, rng=rng)\n",
    "    # self.b[o] is the bias term for the o-th neuron\n",
    "    self.b = random_tensor(output_dim, init=init, rng=rng)\n",
    "\n",
    "  def forward(self, input_: Tensor) -> Tensor:\n",
    "    # Save the input to use in the backward pass.\n",
    "    self.input = input_\n",
    "\n",
    "    # Return the vector of neuron outputs.\n",
    "    return [la.dot(input_, self.w[o]) + self.b[o] \n",
    "            for o in range(self.output_dim)]\n",
    "  \n",
    "  def backward(self, gradient: Tensor) -> Tensor:\n",
    "    # Each b[o] gets added to output[o], whcich means the gradient of b is the \n",
    "    # same as the output gradient\n",
    "    self.b_grad = gradient\n",
    "\n",
    "    # Each w[o][i] multiplies input[i] and gets added to output[o]. So its\n",
    "    # gradient is input[i] * gradient[o].\n",
    "    self.w_grad = [[self.input[i] * gradient[o]\n",
    "                    for i in range(self.input_dim)]\n",
    "                      for o in range(self.output_dim)]\n",
    "\n",
    "    # Each input[i] multiplies every w[o][i] and gets added to every output[o]. \n",
    "    # So its gradient is the sum of w[o][i] * gradient[o] across all the outputs.\n",
    "    return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "            for i in range(self.input_dim)]\n",
    "\n",
    "  def params(self) -> Iterable[Tensor]:\n",
    "    return [self.w, self.b]\n",
    "\n",
    "  def grads(self) -> Iterable[Tensor]:\n",
    "    return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks as a Sequence of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: XOR Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: FizzBuzz Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmaxes and Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
