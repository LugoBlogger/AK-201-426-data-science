{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs   \n",
    "- [2023/03/08]   \n",
    "  Restart this notebook if you change the scratch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Iterable, List, Dict\n",
    "from scratch.linear_algebra import LinearAlgebra as la\n",
    "from scratch.deep_learning import DeepLearning as dl\n",
    "from scratch.deep_learning import \\\n",
    "  Layer, Tensor, Linear, Sequential, SoftmaxCrossEntropy, Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to randomly choose an index based on an arbitrary set of weights\n",
    "# If we give the function with weight [1, 1, 3], then on-fifth of the time\n",
    "# it will return 0, one-fifth of the time it will return 1, and three fifth\n",
    "# of the time it will return 2. Five is from 1 + 1 + 3 = 5.\n",
    "def sample_from(weights: List[float], rng) -> int:\n",
    "  \"\"\"returns i with probability weights[i] / sum(weights)\"\"\" \n",
    "  total = sum(weights)\n",
    "  rnd = total * rng.random()        # uniform between 0 and total\n",
    "  for i, w in enumerate(weights):   \n",
    "    rnd -= w                        # return the smallest i such that\n",
    "    if rnd <= 0:                    # weights[0] + ... + weights[i] >= rnd\n",
    "      return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "  def __init__(self, words: List[str] = None) -> None:\n",
    "    self.w2i: Dict[str, int] = {}   # mapping word -> word_id\n",
    "    self.i2w: Dict[int, str] = {}   # mapping word_id -> word\n",
    "\n",
    "    for word in (words or []):      # If words were provided\n",
    "      self.add(word)                # add them.\n",
    "\n",
    "\n",
    "  @property \n",
    "  def size(self) -> int:\n",
    "    \"\"\"how many words are in the vocabulary\"\"\" \n",
    "    return len(self.w2i)\n",
    "\n",
    "  def add(self, word: str) -> None:\n",
    "    if word not in self.w2i:        # If the word is new to us:\n",
    "      word_id = len(self.w2i)       # Find the next id. \n",
    "      self.w2i[word] = word_id      # Add to the word -> word_id map. \n",
    "      self.i2w[word_id] = word      # Add to the word_id -> word map. \n",
    "\n",
    "  def get_id(self, word: str) -> int: \n",
    "    \"\"\"return the id of the word (or None)\"\"\" \n",
    "    return self.w2i.get(word)\n",
    "\n",
    "  def get_word(self, word_id: int) -> str:\n",
    "    \"\"\"return the word with the given id (or None)\"\"\" \n",
    "    return self.i2w.get(word_id)\n",
    "\n",
    "  def one_hot_encode(self, word: str) -> Tensor:\n",
    "    word_id = self.get_id(word)\n",
    "    assert word_id is not None, f\"unknown word {word}\"\n",
    "\n",
    "    return [1.0 if i == word_id else 0.0 for i in range(self.size)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "This kind of neural network will capture the order of the elements in \n",
    "the input sequence. This is done by introducing *hidden state* that\n",
    "maintains between inputs.\n",
    "\n",
    "**linear layer**\n",
    "```\n",
    "output[o] = dot(w[o], input) + b[o]\n",
    "```\n",
    "\n",
    "**recurrent layer**\n",
    "```\n",
    "output[0] = dot(w[o], input) + dot(u[o], hidden) + b[o]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following layer is a simplest RNN, you should use it in practice.\n",
    " The purpose of this simplest RNN for understanding the concept behind\n",
    " recurrent neural network. More standard RNN is GRU or LSTM that can \n",
    " be generated using TensorFlow or PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRnn(Layer):\n",
    "  \"\"\"Just about the simplest possible recurrent layer.\"\"\" \n",
    "  def __init__(self, input_dim: int, hidden_dim: int, rng) -> None:\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "\n",
    "    self.w = dl.random_tensor(hidden_dim, input_dim, init=\"xavier\", rng=rng)\n",
    "    self.u = dl.random_tensor(hidden_dim, hidden_dim, init=\"xavier\", rng=rng)\n",
    "    self.b = dl.random_normal(hidden_dim, rng=rng)\n",
    "\n",
    "    self.reset_hidden_state()\n",
    "\n",
    "\n",
    "  def reset_hidden_state(self) -> None:\n",
    "    self.hidden = [0 for _ in range(self.hidden_dim)]\n",
    "\n",
    "  \n",
    "  def forward(self, input_: Tensor) -> Tensor:\n",
    "    self.input = input_               # save both input and previous\n",
    "    self.prev_hidden = self.hidden    # hidden state to use in backprop\n",
    "\n",
    "    a = [(la.dot(self.w[h], input_) +         # weights @ input\n",
    "          la.dot(self.u[h], self.hidden) +    # weights @ hidden\n",
    "          self.b[h])                          # bias\n",
    "         for h in range(self.hidden_dim)]\n",
    "\n",
    "    self.hidden = dl.tensor_apply(dl.tanh, a)   # Apply tanh activation\n",
    "    return self.hidden\n",
    "\n",
    "\n",
    "  def backward(self, gradient: Tensor) -> None:\n",
    "    # Backpropagate through the tanh\n",
    "    a_grad = [gradient[h] * (1 - self.hidden[h] ** 2)\n",
    "              for h in range(self.hidden_dim)]\n",
    "\n",
    "    # b has the same gradient as a\n",
    "    self.b_grad = a_grad\n",
    "\n",
    "    # Each w[h][i] is multiplied by input[i] and added to a[h],\n",
    "    # so each w_grad[h][i] = a_grad[h] * input[i]\n",
    "    self.w_grad = [[a_grad[h] * self.input[i]\n",
    "                    for i in range(self.input_dim)]\n",
    "                      for h in range(self.hidden_dim)]\n",
    "\n",
    "    # Eaach u[h][h2] is multiplied by hidden[h2] and added to a[h],\n",
    "    # so each u_grad[h][h2] = a_grad[h] * prev_hidden[h2]\n",
    "    self.u_grad = [[a_grad[h] * self.prev_hidden[h2]\n",
    "                    for h2 in range(self.hidden_dim)]\n",
    "                      for h in range(self.hidden_dim)]\n",
    "\n",
    "    # Each input[i] is multiplied by every w[h][i] and added to a[h],\n",
    "    # so each input_grad[i] = sum(a_grad[h] * w[h][i] for h in ...)\n",
    "    return [sum(a_grad[h] * self.w[h][i] for h in range(self.hidden_dim))\n",
    "            for i in range(self.input_dim)]\n",
    "\n",
    "  def params(self) -> Iterable[Tensor]:\n",
    "    return [self.w, self.u, self.b]\n",
    "\n",
    "  def grads(self) -> Iterable[Tensor]:\n",
    "    return [self.w_grad, self.u_grad, self.b_grad]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using a Character-Level RNN\n",
    "\n",
    "In this example, we want to generate an alternative brand name\n",
    "from the list of top 100 brand names of successful startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ycombinator.com/topcompanies/\"\n",
    "soup = BeautifulSoup(requests.get(url).text, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = [ahref.text for ahref in soup.find_all(\"a\", \"company-name\")]\n",
    "len(companies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a `Vocabulary` from the characters names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 't',\n",
       " 'r',\n",
       " 'i',\n",
       " 'p',\n",
       " 'e',\n",
       " 'I',\n",
       " 'n',\n",
       " 's',\n",
       " 'a',\n",
       " 'c',\n",
       " 'O',\n",
       " 'F',\n",
       " 'B',\n",
       " 'x',\n",
       " 'D',\n",
       " 'l',\n",
       " 'R',\n",
       " 'g',\n",
       " 'G',\n",
       " 'u',\n",
       " 'o',\n",
       " 'z',\n",
       " 'y',\n",
       " ' ',\n",
       " 'A',\n",
       " 'h',\n",
       " 'v',\n",
       " 'C',\n",
       " 'k',\n",
       " 'Z',\n",
       " 'M',\n",
       " 'W',\n",
       " 'b',\n",
       " 'f',\n",
       " 'w',\n",
       " 'd',\n",
       " 'E',\n",
       " 'q',\n",
       " 'm',\n",
       " 'T',\n",
       " 'P',\n",
       " 'X',\n",
       " 'L',\n",
       " 'H',\n",
       " 'N',\n",
       " '1',\n",
       " 'J',\n",
       " 'V',\n",
       " '.',\n",
       " 'Q',\n",
       " 'j',\n",
       " 'K',\n",
       " '9',\n",
       " 'Y',\n",
       " \"'\",\n",
       " ',',\n",
       " 'U',\n",
       " '-',\n",
       " '6',\n",
       " '4',\n",
       " '5',\n",
       " '3',\n",
       " '0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary([c for company in companies \n",
    "                        for c in company])\n",
    "\n",
    "[vocab.get_word(id_) for id_ in range(vocab.size)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a token to signify the start and end of the company names.\n",
    "Luckily these start and stop characters do not appear in the company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '^'\n",
    "STOP = \"$\"\n",
    "\n",
    "vocab.add(START)\n",
    "vocab.add(STOP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "We'll one-hot-encode each character, passit thorugh two `SimpleRnn`s,\n",
    "and then use a `Linear` layer to generate the scores for each possible next\n",
    "character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023_05_04\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "HIDDEN_DIM = 32   # You should experiment with different sizes!\n",
    "\n",
    "rnn1 = SimpleRnn(input_dim=vocab.size, hidden_dim=HIDDEN_DIM, rng=rng)\n",
    "rnn2 = SimpleRnn(input_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM, rng=rng)\n",
    "linear = Linear(input_dim=HIDDEN_DIM, output_dim=vocab.size, rng=rng)\n",
    "\n",
    "model = Sequential([\n",
    "  rnn1,\n",
    "  rnn2,\n",
    "  linear])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the company name\n",
    "def generate(seed: str = START, max_len: int = 50, rng=None) -> str:\n",
    "  rnn1.reset_hidden_state()     # Reset both hidden states\n",
    "  rnn2.reset_hidden_state()\n",
    "  output = [seed]               # Start the output with the specified seed\n",
    "\n",
    "  # Keep going until we produce the STOP character or reach the max length\n",
    "  while output[-1] != STOP and len(output) < max_len:\n",
    "    # Use the last character as the input\n",
    "    input_ = vocab.one_hot_encode(output[-1])\n",
    "\n",
    "    # Generate scores using the model\n",
    "    predicted = model.forward(input_)\n",
    "\n",
    "    # Convert them to probabilities and draw a random char_id\n",
    "    probabilities = dl.softmax(predicted)\n",
    "    next_char_id = sample_from(probabilities, rng)\n",
    "\n",
    "    # Add the corresponding char to our output\n",
    "    output.append(vocab.get_word(next_char_id))\n",
    "\n",
    "  # Get rid of START and STOP characters and return the word\n",
    "  return ''.join(output[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:55<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17749.315081392997 CrsSdinnctii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:55<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 17205.134826262944 Hayadnat S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:54<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 16976.42295470643 AeraakTie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:54<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 16761.137104912366 KNbir dIreEynhc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:54<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16530.228342236547 Luaee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:53<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 16313.810636737939 Iabir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:54<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 16048.610374360136 MagyZhilt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:54<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 15878.46506469731 TibNonb Pererh calefarmacigy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 15707.896646430572 SecRiid Lcamt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 15543.65237800709 Nponan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 15410.46875921924 ZliltoRine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 15278.191902133607 Rarbirey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 15090.015538184232 Grerlardens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 14944.983467231837 Senviik\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14750.24727519621 Tabt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 14594.525716175089 Daivinelimidancire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 14450.734230731441 KeduR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 14290.176549952672 Fala\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 14105.790121997261 Rasalash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 13958.159612211182 Stfnty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 13841.315178819561 Abbe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 13728.607953012666 Bbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 13613.747900359842 Axsre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 13513.038698170905 Henogren\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 13387.603740066437 Dinely\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 13284.73520279218 Atass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 13124.253405500338 Bundog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 13095.517439819552 Curmiwe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 13059.838076110396 Srummc.ing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 12898.507629397722 Assgivls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:52<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 12751.94028237859 Ippel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:53<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 12625.980017250073 Herdo Bwomis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:53<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 12543.273510088175 Eaghtremderees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:56<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 12517.958766865482 6elmom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:53<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 12432.23234620826 Rolly,al\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:54<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 12392.806565012545 6kume Cliat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:55<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 12282.6275524506 Softvoma Haii Hex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [00:53<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 12244.064330844303 Debofut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 125/586 [00:11<00:39, 11.67it/s]"
     ]
    }
   ],
   "source": [
    "loss = SoftmaxCrossEntropy()\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Total training time 300 minutes \n",
    "for epoch in range(300):\n",
    "  rng.shuffle(companies)      # Train in a differnt order each epoch. \n",
    "  epoch_loss = 0              # Track the loss. \n",
    "\n",
    "  for company in tqdm.tqdm(companies):\n",
    "    rnn1.reset_hidden_state()     # Reset both hidden states.\n",
    "    rnn2.reset_hidden_state() \n",
    "    company = START + company + STOP    # Add START and STOP characters\n",
    "\n",
    "    # The rest is just our usual training loop, except that the inputs\n",
    "    # and target are the one-hot-encoded previous and next characters.\n",
    "    for prev, next_ in zip(company, company[1:]):\n",
    "      input_ = vocab.one_hot_encode(prev)\n",
    "      target = vocab.one_hot_encode(next_)\n",
    "      predicted = model.forward(input_)\n",
    "      epoch_loss += loss.loss(predicted, target)\n",
    "      gradient = loss.gradient(predicted, target)\n",
    "      model.backward(gradient)\n",
    "      optimizer.step(model)\n",
    "\n",
    "  # Each epoch, print the loss and also generate a name.\n",
    "  print(epoch, epoch_loss, generate(rng=rng))\n",
    "\n",
    "  # Turn down the learning rate for the last 100 epochs.\n",
    "  # There's no principled reason for this, but it seems to work.\n",
    "  if epoch == 200:\n",
    "    optimizer.lr *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
