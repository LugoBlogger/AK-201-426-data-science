{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs\n",
    "- [2024/04/30]   \n",
    "  You do not need to restart this notebook when updating the scratch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from typing import NamedTuple, List\n",
    "from scratch.linear_algebra import LinearAlgebra as la\n",
    "from scratch.working_with_data import DimReduction\n",
    "from scratch.deep_learning import DeepLearning as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({\n",
    "  'font.size': 16,\n",
    "  'grid.alpha': 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This points to the current directory, modify if your files are elsewhere.\n",
    "path_to_file = \"./datasets/ml-100k/\"\n",
    "MOVIES = path_to_file + \"u.item\"   # pipe-delimited: movie_id|title|...\n",
    "RATINGS = path_to_file + \"u.data\"  # tab-delimited: user_id, movie_id, rating, timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `Rating` class to make things easier when handling with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(NamedTuple):\n",
    "  user_id: str\n",
    "  movie_id: str \n",
    "  rating: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `movie_id` and `user_id` are actually integers, but they're not    \n",
    "> consecutive, which means if we worked with them as integers we'd end up with  \n",
    "> a lot of wasted dimensions (unless we renumbered everything). So to keep it    \n",
    "> simpler we'll just treat them as strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify this encoding to avoid a UnicodeDecodeError.\n",
    "# See: https://stackoverflow.com/a/53136168/1076346 -- Encoding issues while reading/importing CSV file in Python3 Pandas\n",
    "with open(MOVIES, encoding=\"iso-8859-1\") as fp:\n",
    "  reader = csv.reader(fp, delimiter=\"|\")\n",
    "  movies = {movie_id: title for movie_id, title, *_ in reader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of [Rating]\n",
    "with open(RATINGS, encoding=\"iso-8859-1\") as fp:\n",
    "  reader = csv.reader(fp, delimiter=\"\\t\")\n",
    "  ratings = [Rating(user_id, movie_id, float(rating))\n",
    "              for user_id, movie_id, rating, _ in reader]\n",
    "\n",
    "# 1682 movies rated by 943 users\n",
    "assert len(movies) == 1682\n",
    "assert len(list({rating.user_id for rating in ratings})) == 943"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An exploratory data analysis:\n",
    "- The average ratings for _Star Wars_ movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36 Star Wars (1977)\n",
      "4.20 Empire Strikes Back, The (1980)\n",
      "4.01 Return of the Jedi (1983)\n"
     ]
    }
   ],
   "source": [
    "# Data structure for accumulating ratings by movie_id\n",
    "star_wars_ratings = {movie_id: [] for movie_id, title in movies.items()\n",
    "                      if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
    "\n",
    "# Iterate over ratings, accumulating the Star Wars ones\n",
    "for rating in ratings:\n",
    "  if rating.movie_id in star_wars_ratings:\n",
    "    star_wars_ratings[rating.movie_id].append(rating.rating)\n",
    "\n",
    "# Compute the average rating for each movie\n",
    "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
    "                for movie_id, title_ratings in star_wars_ratings.items()]\n",
    "\n",
    "# And then print them in order\n",
    "for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
    "  print(f\"{avg_rating:.2f} {movies[movie_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's come back to the datasets `movies` and `ratings`. We want to try to come  \n",
    "up with a a model to predict the ratings. First, we split the ratings data into  \n",
    "train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24_04_30\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# We re-run ratings data creation to avoid run several times of in-place shuffling\n",
    "# Create a list of [Rating]\n",
    "with open(RATINGS, encoding=\"iso-8859-1\") as fp:\n",
    "  reader = csv.reader(fp, delimiter=\"\\t\")\n",
    "  ratings = [Rating(user_id, movie_id, float(rating))\n",
    "              for user_id, movie_id, rating, _ in reader]\n",
    "\n",
    "rng.shuffle(ratings)\n",
    "\n",
    "split1 = int(len(ratings) * 0.7)\n",
    "split2 = int(len(ratings) * 0.85)\n",
    "\n",
    "train = ratings[:split1]              # 70% of the data\n",
    "validation = ratings[split1:split2]   # 15% of the data\n",
    "test = ratings[split2:]               # 15% of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to have a simple baseline model and make sure that ours  \n",
    "constructed model does better than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating = sum(rating.rating for rating in train) / len(train)\n",
    "baseline_error = sum((rating.rating - avg_rating) ** 2\n",
    "                      for rating in test) / len(test)\n",
    "\n",
    "# This is what we hope to do better than\n",
    "assert 1.26 < baseline_error < 1.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating embeddings for users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24_04_30\n",
    "rng = np.random.default_rng(seed)\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "# Find unique ids\n",
    "user_ids = {rating.user_id for rating in ratings}\n",
    "movie_ids = {rating.movie_id for rating in ratings}\n",
    "\n",
    "# Then create a random vector per id\n",
    "user_vectors = {user_id: dl.random_tensor(EMBEDDING_DIM, rng=rng) \n",
    "                for user_id in user_ids}\n",
    "movie_vectors = {movie_id: dl.random_tensor(EMBEDDING_DIM, rng=rng) \n",
    "                  for movie_id in movie_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Review in dictionary]   \n",
    "There is a subtle updating value of dictionary when the dictionary is    \n",
    "querying and assigning to a new variable. Changing this new variable will    \n",
    "affect the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list as a value of a dictionary\n",
      "a_dict: {'k1': 2, 'k2': [5, 6]}\n",
      "a_dict: {'k1': 2, 'k2': [-2, 6]}\n",
      "\n",
      "A numpy array as a value of a dictionary\n",
      "a_dict_with_numpy: {'k1': 2, 'k2': array([5, 6])}\n",
      "a_dict_with_numpy: {'k1': 2, 'k2': array([-2,  6])}\n"
     ]
    }
   ],
   "source": [
    "print(\"A list as a value of a dictionary\")\n",
    "a_dict = {\"k1\": 2, \"k2\": [5, 6]}\n",
    "print(f\"a_dict: {a_dict}\")\n",
    "\n",
    "b_list = a_dict[\"k2\"]   # pass by reference not by value!\n",
    "b_list[0] = -2\n",
    "\n",
    "print(f\"a_dict: {a_dict}\")\n",
    "\n",
    "print(\"\\nA numpy array as a value of a dictionary\")\n",
    "a_dict_with_numpy = {\"k1\": 2, \"k2\": np.array([5, 6])}\n",
    "print(f\"a_dict_with_numpy: {a_dict_with_numpy}\")\n",
    "\n",
    "b_numpy = a_dict_with_numpy[\"k2\"]\n",
    "b_numpy[0] = -2\n",
    "print(f\"a_dict_with_numpy: {a_dict_with_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a training loop for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(dataset: List[Rating], learning_rate: float = None) -> None:\n",
    "  with tqdm.tqdm(dataset) as t:\n",
    "    loss = 0.0\n",
    "    for i, rating in enumerate(t):\n",
    "      movie_vector = movie_vectors[rating.movie_id]  # this is a copy by reference\n",
    "      user_vector = user_vectors[rating.user_id]     # this is a copy by reference\n",
    "      predicted = la.dot(user_vector, movie_vector)\n",
    "      error = predicted - rating.rating\n",
    "      loss += error ** 2 \n",
    "\n",
    "      if learning_rate is not None:\n",
    "        #   predicted = m_0 * u_0 + ... + m_k * u_k\n",
    "        # So each u_j enters output with coefficient m_j\n",
    "        # and each m_j enters output with coefficient u_j\n",
    "        user_gradient = [error * m_j for m_j in movie_vector]\n",
    "        movie_gradient = [error * u_j for u_j in user_vector]\n",
    "\n",
    "        # Take gradient steps\n",
    "        # -- [Note]: updating user_vector and movie_vector will change\n",
    "        #            user_vectors and movie_vectors\n",
    "        for j in range(EMBEDDING_DIM):\n",
    "          user_vector[j] -= learning_rate * user_gradient[j]   \n",
    "          movie_vector[j] -= learning_rate * movie_gradient[j]\n",
    "      \n",
    "      t.set_description(f\"avg loss: {loss / (i + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.045000000000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 17.023555580112564:   0%|          | 0/70000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 6.114683603097698: 100%|██████████| 70000/70000 [00:27<00:00, 2529.02it/s] \n",
      "avg loss: 1.2843711989112063: 100%|██████████| 15000/15000 [00:05<00:00, 2586.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.04050000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 1.1314823025058312: 100%|██████████| 70000/70000 [00:27<00:00, 2525.47it/s]\n",
      "avg loss: 1.0864504499259353: 100%|██████████| 15000/15000 [00:05<00:00, 2546.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.03645000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 1.0234978405687394: 100%|██████████| 70000/70000 [00:27<00:00, 2547.25it/s]\n",
      "avg loss: 1.0390623703356796: 100%|██████████| 15000/15000 [00:05<00:00, 2605.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.03280500000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9812651379818479: 100%|██████████| 70000/70000 [00:27<00:00, 2552.21it/s]\n",
      "avg loss: 1.0120019862066691: 100%|██████████| 15000/15000 [00:05<00:00, 2575.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.02952450000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9529818585920801: 100%|██████████| 70000/70000 [00:27<00:00, 2554.18it/s]\n",
      "avg loss: 0.9920729800925361: 100%|██████████| 15000/15000 [00:05<00:00, 2610.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.02657205000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9304333465645488: 100%|██████████| 70000/70000 [00:27<00:00, 2550.56it/s]\n",
      "avg loss: 0.9759284893406273: 100%|██████████| 15000/15000 [00:06<00:00, 2476.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.02391484500000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9113048797855346: 100%|██████████| 70000/70000 [00:28<00:00, 2474.94it/s]\n",
      "avg loss: 0.9624217771363592: 100%|██████████| 15000/15000 [00:05<00:00, 2557.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.021523360500000012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8947573917030615: 100%|██████████| 70000/70000 [00:27<00:00, 2546.69it/s]\n",
      "avg loss: 0.9510325926720634: 100%|██████████| 15000/15000 [00:05<00:00, 2594.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.01937102445000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8803588449760007: 100%|██████████| 70000/70000 [00:27<00:00, 2563.54it/s]\n",
      "avg loss: 0.9414253683962088: 100%|██████████| 15000/15000 [00:05<00:00, 2526.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.01743392200500001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8677913233198074: 100%|██████████| 70000/70000 [00:29<00:00, 2374.94it/s]\n",
      "avg loss: 0.9333298474113576: 100%|██████████| 15000/15000 [00:05<00:00, 2508.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.015690529804500006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8567849026342591: 100%|██████████| 70000/70000 [00:27<00:00, 2535.68it/s]\n",
      "avg loss: 0.9265141135003028: 100%|██████████| 15000/15000 [00:05<00:00, 2535.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.014121476824050006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8471076704714128: 100%|██████████| 70000/70000 [00:28<00:00, 2458.37it/s]\n",
      "avg loss: 0.9207784539236817: 100%|██████████| 15000/15000 [00:07<00:00, 2058.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.012709329141645007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8385635141800183: 100%|██████████| 70000/70000 [00:28<00:00, 2441.93it/s]\n",
      "avg loss: 0.9159520617023131: 100%|██████████| 15000/15000 [00:05<00:00, 2560.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.011438396227480507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8309886155551292: 100%|██████████| 70000/70000 [00:27<00:00, 2543.00it/s]\n",
      "avg loss: 0.9118896538698553: 100%|██████████| 15000/15000 [00:05<00:00, 2567.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.010294556604732457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8242468213549047: 100%|██████████| 70000/70000 [00:27<00:00, 2511.77it/s]\n",
      "avg loss: 0.9084681200329222: 100%|██████████| 15000/15000 [00:05<00:00, 2562.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.00926510094425921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8182249721231604: 100%|██████████| 70000/70000 [00:27<00:00, 2533.53it/s]\n",
      "avg loss: 0.9055835090232455: 100%|██████████| 15000/15000 [00:06<00:00, 2461.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.00833859084983329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8128287614251115: 100%|██████████| 70000/70000 [00:28<00:00, 2478.52it/s]\n",
      "avg loss: 0.9031484346128095: 100%|██████████| 15000/15000 [00:05<00:00, 2560.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.007504731764849962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8079792798211634: 100%|██████████| 70000/70000 [00:28<00:00, 2455.48it/s]\n",
      "avg loss: 0.9010898223692646: 100%|██████████| 15000/15000 [00:05<00:00, 2579.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.006754258588364966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8036102026445161: 100%|██████████| 70000/70000 [00:31<00:00, 2190.86it/s]\n",
      "avg loss: 0.8993468794712797: 100%|██████████| 15000/15000 [00:10<00:00, 1462.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.00607883272952847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.7996655168510675: 100%|██████████| 70000/70000 [00:47<00:00, 1460.24it/s]\n",
      "avg loss: 0.8978692104365604: 100%|██████████| 15000/15000 [00:10<00:00, 1377.83it/s]\n",
      "avg loss: 0.8992023894713554: 100%|██████████| 15000/15000 [00:05<00:00, 2731.03it/s]\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 20    #  12 minutes and take a lot of resources\n",
    "learning_rate = 0.05\n",
    "for epoch in range(N_epoch):\n",
    "  learning_rate *= 0.9\n",
    "  print(epoch, learning_rate)\n",
    "  loop(train, learning_rate=learning_rate)\n",
    "  loop(validation)\n",
    "\n",
    "loop(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use principal component analysis to inspect the learned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dv: 4454.975: 100%|██████████| 100/100 [00:00<00:00, 206.05it/s]\n",
      "dv: 986.594: 100%|██████████| 100/100 [00:00<00:00, 214.80it/s]\n"
     ]
    }
   ],
   "source": [
    "original_vectors = [vector for vector in movie_vectors.values()]\n",
    "components = DimReduction.pca(original_vectors, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our vectors to represent the principal components and join in   \n",
    "the movie IDs and average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_by_movie = defaultdict(list)\n",
    "for rating in ratings:\n",
    "  ratings_by_movie[rating.movie_id].append(rating.rating)\n",
    "\n",
    "vectors = [\n",
    "  (movie_id, sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]), \n",
    "  movies[movie_id], \n",
    "  vector)\n",
    "  for movie_id, vector in zip(movie_vectors.keys(),\n",
    "                              transform(original_vectors, components))\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
