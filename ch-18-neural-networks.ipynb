{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs\n",
    "- [2024/04/15]   \n",
    "  You do not need to restart this notebook when updating the scratch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from scratch.linear_algebra import LinearAlgebra as la\n",
    "from scratch.linear_algebra import Vector\n",
    "from scratch.gradient_descent import GradientDescent as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural networks (or neural network for short) is a predictive  \n",
    "model motivated by the way the brain operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is a mathematical model of a single neuron with $n$ binary inputs.  \n",
    "An example of 3-binary inputs:\n",
    "- [0, 0, 0]\n",
    "- [0, 0, 1]\n",
    "- [0, 1, 0]\n",
    "- [0, 1, 1]\n",
    "- [1, 0, 0]\n",
    "- ...\n",
    "- [1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a perceptron using Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x: float) -> float:\n",
    "  return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "  \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "  calculation = la.dot(weights, x) + bias\n",
    "  return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of perceptron formula above can be seen as a hyperplane of points\n",
    "$\\mathbf{x}$\n",
    "$$\n",
    "  \\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
    "$$\n",
    "where $\\mathbf{w}$ is the weight (vector), $\\mathbf{x}$ is the input (vector),   \n",
    "and $b$ is the bias term (scalar). Then perceptron is simply distinguishing   \n",
    "between the half-spaces separated by the hyperplane above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the perceptron above, we can build a function that can solve AND gate problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
    "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also solve OR gate and NOT gate with the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
    "assert perceptron_output(not_weights, not_bias, [1]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, the perceptron above cannot solve XOR gate no matter how we set weights   \n",
    "and bias. Because of that we need more complicated neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a neuron model by using perceptron and a nonlinear function.  \n",
    "There are two options to be used as a nonlinear function, because we want to\n",
    "represent activation and no-activation of neuron:\n",
    "- with `step_function` (but there is a problem that this function is not continuous)\n",
    "- with `sigmoid` (this is an approximation of step function but this function\n",
    "  is continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our model of neuron simple, we concatenate 1 to `inputs` vector and put   \n",
    "the bias into weights vector as a\n",
    "$$\n",
    "  \\text{neuron}(\\mathbf{x};\\mathbf{w}, b)\n",
    "  = \n",
    "  \\begin{bmatrix}\n",
    "    w_1 & w_2 & \\ldots & w_n & b\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate another function to make our model become a nonlinear model   \n",
    "using sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t: float) -> float:\n",
    "  return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "  # weights includes the bias term, inputs includes a a\n",
    "  return sigmoid(la.dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent a neural network as a list (layers) of lists (neurons) of  \n",
    "vectors (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                  input_vector: Vector) -> List[Vector]:\n",
    "  \"\"\"Feeds the input vector through the neural network. Returns the outputs of\n",
    "  all layers (not just the last one) \"\"\"\n",
    "  outputs: List[Vector] = []\n",
    "\n",
    "  for layer in neural_network:\n",
    "    input_with_bias = input_vector + [1]              # Add a constant.\n",
    "    output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "              for neuron in layer]                    # for each neuron.\n",
    "    outputs.append(output)                            # Add to results.\n",
    "\n",
    "    # Then the input to the next layer is the output of this one\n",
    "    input_vector = output\n",
    "  \n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_network = [                    # hidden layer\n",
    "                [[20., 20, -30],   # - `and` neuron\n",
    "                 [20., 20, -10]],  # - `or` neuron\n",
    "                                   # output layer\n",
    "                [[-60., 60, -30]]  # - `2nd input but not 1st input` neuron\n",
    "              ]\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n",
    "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n",
    "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n",
    "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above feed-forward neural network can be understood by the following diagram\n",
    "\n",
    "<img src=\"./img-resources/neural-nets-feed-forward.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above feed-forward neural network for solving XOR gate, we defined   \n",
    "by hand neural network parameters. Now we want to define neural networks such   \n",
    "that we can automatically get the neural networks parameters.   \n",
    "This process is called _training_. To perform training, we need a training set.  \n",
    "For our XOR gate, the training set is\n",
    "\n",
    "| `input`  | `output` |\n",
    "|----------|----------|\n",
    "| `[0, 0]` | `[1]`    |\n",
    "| `[0, 1]` | `[1]`    |\n",
    "| `[1, 0]` | `[1]`    |\n",
    "| `[1, 1]` | `[0]`    |\n",
    "\n",
    "During training, we adjust weights by the following algorithm\n",
    "1. Run `feed_forward` on an input vector to produce the outputs of all the   \n",
    "   neurons in the network.\n",
    "2. We know the target output, so we can compute a _loss_ that's the (half)  \n",
    "   of the sum of the squared erros. [In textbook, the author do not say _half_.   \n",
    "   If we do not put the half, the formula for variable `output_deltas` in    \n",
    "   `sqerror_gradients` must include a factor 2]\n",
    "3. Compute the gradient of this loss as a function of the output neuron's weights\n",
    "4. \"Propagate\" the gradients and errors backward to compute the gradients with  \n",
    "   respect to the hidden neuron's weights.\n",
    "5. Take a gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a look to [`neural_nets.drawio`](./img-resources/neural-nets.drawio) \n",
    "for the derivation of the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]], input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "  \"\"\"Given a neural network, an input vector, and a target vector,\n",
    "  make a prediction and compute the gradient of the squared error loss with \n",
    "  respect to the neuron weights\"\"\"\n",
    "\n",
    "  # forward pass\n",
    "  hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "  # gradients with respect to output neuron pre-activation outputs\n",
    "  output_deltas = [output * (1 - output) * (output - target)\n",
    "                    for output, target in zip(outputs, target_vector)]\n",
    "  \n",
    "  # print(f\"len(output_deltas): {len(output_deltas)}\")\n",
    "  # print(f\"len(network[-1]): {len(network[-1])}\")\n",
    "  # gradients with respect to output neuron weights\n",
    "  output_grads = [[output_deltas[i] * hidden_output\n",
    "                    for hidden_output in hidden_outputs + [1]]\n",
    "                      for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "  # gradients with respect to hidden neuron pre-activation outputs\n",
    "  hidden_deltas = [hidden_output * (1 - hidden_output) * \n",
    "                    la.dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                    for i, hidden_output in enumerate(hidden_outputs)]\n",
    "  \n",
    "  # gradients with respect to hidden neuron weights\n",
    "  hidden_grads = [[hidden_deltas[i] * input_ for input_ in input_vector + [1]]\n",
    "                  for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "  return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for checking the dimension of output_deltas and network[-1]\n",
    "# seed = 24_04_25\n",
    "# rng = np.random.default_rng(seed)\n",
    "\n",
    "# # training data\n",
    "# xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "# ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# # start with random weights\n",
    "# network = [                                           # hidden layer: 2 inputs -> 2 outputs\n",
    "#             [[rng.random() for _ in range(2 + 1)],    # - 1st hidden neuron\n",
    "#              [rng.random() for _ in range(2 + 1)]],   # - 2nd hidden neuron\n",
    "#                                                       # output layer: 2 inputs -> 1 output\n",
    "#             [[rng.random() for _ in range(2 + 1)]]    # - 1st output neuron\n",
    "# ]\n",
    "\n",
    "# sqerror_gradients(network, xs[0], ys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to learn the XOR network. We will start by generating the training  \n",
    "data and initializing our neural network with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor:   0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|██████████| 20000/20000 [00:01<00:00, 17035.30it/s]\n"
     ]
    }
   ],
   "source": [
    "seed = 24_04_25\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# start with random weights\n",
    "network = [                                           # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[rng.random() for _ in range(2 + 1)],    # - 1st hidden neuron\n",
    "             [rng.random() for _ in range(2 + 1)]],   # - 2nd hidden neuron\n",
    "                                                      # output layer: 2 inputs -> 1 output\n",
    "            [[rng.random() for _ in range(2 + 1)]]    # - 1st output neuron\n",
    "]\n",
    "init_network = network.copy()\n",
    "\n",
    "learning_rate = 1.0     # if you use loss function 0.5*|| out - target ||^2\n",
    "# learning_rate = 0.5   # if you use loss function || out - target ||^2\n",
    "\n",
    "for epoch in tqdm.trange(20_000, desc=\"neural net for xor\"):\n",
    "  for x, y in zip(xs, ys):\n",
    "    gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "    # Take a gradient step for each neuron in each layer\n",
    "    network = [[gd.gradient_step(neuron, grad, -learning_rate)\n",
    "                for neuron, grad in zip(layer, layer_grad)]\n",
    "                  for layer, layer_grad in zip(network, gradients)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it learned XOR\n",
    "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 1])[-1][0] < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.027046326868200243, 0.9582945652249892], [0.008066210168484208]]\n",
      "[[2.828055316890695e-05, 0.04268833549811431], [0.9907825583757307]]\n",
      "[[0.9580605062851266, 0.9999295783071573], [0.992432080159848]]\n",
      "[[0.02271307293959756, 0.9649808000792041], [0.007170531277044988]]\n"
     ]
    }
   ],
   "source": [
    "for x1, x2 in xs:\n",
    "  print(feed_forward(network, [x1, x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.8748572217914731, 0.2333282645901471, 0.29831543557166096],\n",
       "  [0.5298359741305657, 0.6033836721509632, 0.3441243903330605]],\n",
       " [[0.6883007269360694, 0.7843383033775261, 0.5413104665657402]]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[[6.711468244221999, -6.890522612714037, -3.5827852630319876],\n",
       "  [6.426415021705044, -6.2447270565192365, 3.134523759418311]],\n",
       " [[10.883927950893137, -10.685196688788292, 5.133223033529587]]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(init_network)\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fizz Buzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
