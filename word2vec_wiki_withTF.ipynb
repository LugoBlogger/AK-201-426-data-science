{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs:\n",
    "- [2024/04/27]   \n",
    "  A copy of `word2vec_wiki.ipynb` but usin TensorFlow instead writing all    \n",
    "  the procedures from scratch  \n",
    "\n",
    "- [2024/04/28]   \n",
    "  All the procedures to create a training data set and the model are available   \n",
    "  in [`word2vec` TensorFlow tutorial](https://www.tensorflow.org/text/tutorials/word2vec#vectorize_an_example_sentence)\n",
    "\n",
    "  Using wiki article produced vectors that has no apparent clusters\n",
    "\n",
    "\n",
    "To do:\n",
    "- I would like to copy `ch-21-nlp-02-wordVec.ipynb` with TensorFlow, but    \n",
    "  it is difficult than I thought. We can achieve this by creating a model   \n",
    "  with a custom gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf   # pip install tensorflow\n",
    "import requests \n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "from scratch.deep_learning import Tensor\n",
    "from scratch.word2vec import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of `word2vec` for a Wikipedia article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'academic',\n",
       " 'field',\n",
       " '1',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'statistics',\n",
       " 'scientific',\n",
       " 'computing',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'processes',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'systems',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'or',\n",
       " 'extrapolate',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'potentially',\n",
       " 'noisy',\n",
       " 'structured',\n",
       " 'or',\n",
       " 'unstructured',\n",
       " 'data',\n",
       " '.',\n",
       " '2',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'also',\n",
       " 'integrates',\n",
       " 'domain',\n",
       " 'knowledge',\n",
       " 'from',\n",
       " 'the',\n",
       " 'underlying',\n",
       " 'application',\n",
       " 'domain',\n",
       " 'e',\n",
       " '.',\n",
       " 'g',\n",
       " '.',\n",
       " 'natural',\n",
       " 'sciences',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'medicine',\n",
       " '.',\n",
       " '3',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'multifaceted',\n",
       " 'and',\n",
       " 'can',\n",
       " 'be',\n",
       " 'described',\n",
       " 'as',\n",
       " 'a',\n",
       " 'science',\n",
       " 'a',\n",
       " 'research',\n",
       " 'paradigm',\n",
       " 'a',\n",
       " 'research',\n",
       " 'method',\n",
       " 'a',\n",
       " 'discipline',\n",
       " 'a',\n",
       " 'workflow',\n",
       " 'and',\n",
       " 'a',\n",
       " 'profession',\n",
       " '.',\n",
       " '4',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'a',\n",
       " 'concept',\n",
       " 'to',\n",
       " 'unify',\n",
       " 'statistics',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'informatics',\n",
       " 'and',\n",
       " 'their',\n",
       " 'related',\n",
       " 'methods',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'actual',\n",
       " 'phenomena',\n",
       " 'with',\n",
       " 'data',\n",
       " '.',\n",
       " '5',\n",
       " 'It',\n",
       " 'uses',\n",
       " 'techniques',\n",
       " 'and',\n",
       " 'theories',\n",
       " 'drawn',\n",
       " 'from',\n",
       " 'many',\n",
       " 'fields',\n",
       " 'within',\n",
       " 'the',\n",
       " 'context',\n",
       " 'of',\n",
       " 'mathematics',\n",
       " 'statistics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'information',\n",
       " 'science',\n",
       " 'and',\n",
       " 'domain',\n",
       " 'knowledge',\n",
       " '.',\n",
       " '6',\n",
       " 'However',\n",
       " 'data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'different',\n",
       " 'from',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'and',\n",
       " 'information',\n",
       " 'science',\n",
       " '.',\n",
       " 'Turing',\n",
       " 'Award',\n",
       " 'winner',\n",
       " 'Jim',\n",
       " 'Gray',\n",
       " 'imagined',\n",
       " 'data',\n",
       " 'science',\n",
       " 'as',\n",
       " 'a',\n",
       " 'fourth',\n",
       " 'paradigm',\n",
       " 'of',\n",
       " 'science',\n",
       " 'empirical',\n",
       " 'theoretical',\n",
       " 'computational',\n",
       " 'and',\n",
       " 'now',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'and',\n",
       " 'asserted',\n",
       " 'that',\n",
       " 'everything',\n",
       " 'about',\n",
       " 'science',\n",
       " 'is',\n",
       " 'changing',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'the',\n",
       " 'data',\n",
       " 'deluge',\n",
       " '.',\n",
       " '7',\n",
       " '8',\n",
       " 'A',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'is',\n",
       " 'a',\n",
       " 'professional',\n",
       " 'who',\n",
       " 'creates',\n",
       " 'programming',\n",
       " 'code',\n",
       " 'and',\n",
       " 'combines',\n",
       " 'it',\n",
       " 'with',\n",
       " 'statistical',\n",
       " 'knowledge',\n",
       " 'to',\n",
       " 'create',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'data',\n",
       " '.',\n",
       " '9',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " '10',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'extracting',\n",
       " 'knowledge',\n",
       " 'from',\n",
       " 'typically',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'and',\n",
       " 'applying',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'that',\n",
       " 'data',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'problems',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'application',\n",
       " 'domains',\n",
       " '.',\n",
       " 'The',\n",
       " 'field',\n",
       " 'encompasses',\n",
       " 'preparing',\n",
       " 'data',\n",
       " 'for',\n",
       " 'analysis',\n",
       " 'formulating',\n",
       " 'data',\n",
       " 'science',\n",
       " 'problems',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'developing',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'solutions',\n",
       " 'and',\n",
       " 'presenting',\n",
       " 'findings',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'high',\n",
       " 'level',\n",
       " 'decisions',\n",
       " 'in',\n",
       " 'a',\n",
       " 'broad',\n",
       " 'range',\n",
       " 'of',\n",
       " 'application',\n",
       " 'domains',\n",
       " '.',\n",
       " 'As',\n",
       " 'such',\n",
       " 'it',\n",
       " 'incorporates',\n",
       " 'skills',\n",
       " 'from',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'statistics',\n",
       " 'information',\n",
       " 'science',\n",
       " 'mathematics',\n",
       " 'data',\n",
       " 'visualization',\n",
       " 'information',\n",
       " 'visualization',\n",
       " 'data',\n",
       " 'sonification',\n",
       " 'data',\n",
       " 'integration',\n",
       " 'graphic',\n",
       " 'design',\n",
       " 'complex',\n",
       " 'systems',\n",
       " 'communication',\n",
       " 'and',\n",
       " 'business',\n",
       " '.',\n",
       " '11',\n",
       " '12',\n",
       " 'Statistician',\n",
       " 'Nathan',\n",
       " 'Yau',\n",
       " 'drawing',\n",
       " 'on',\n",
       " 'Ben',\n",
       " 'Fry',\n",
       " 'also',\n",
       " 'links',\n",
       " 'data',\n",
       " 'science',\n",
       " 'to',\n",
       " 'human',\n",
       " 'computer',\n",
       " 'interaction',\n",
       " 'users',\n",
       " 'should',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'intuitively',\n",
       " 'control',\n",
       " 'and',\n",
       " 'explore',\n",
       " 'data',\n",
       " '.',\n",
       " '13',\n",
       " '14',\n",
       " 'In',\n",
       " '2015',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Statistical',\n",
       " 'Association',\n",
       " 'identified',\n",
       " 'database',\n",
       " 'management',\n",
       " 'statistics',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'distributed',\n",
       " 'and',\n",
       " 'parallel',\n",
       " 'systems',\n",
       " 'as',\n",
       " 'the',\n",
       " 'three',\n",
       " 'emerging',\n",
       " 'foundational',\n",
       " 'professional',\n",
       " 'communities',\n",
       " '.',\n",
       " '15',\n",
       " 'Many',\n",
       " 'statisticians',\n",
       " 'including',\n",
       " 'Nate',\n",
       " 'Silver',\n",
       " 'have',\n",
       " 'argued',\n",
       " 'that',\n",
       " 'data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'new',\n",
       " 'field',\n",
       " 'but',\n",
       " 'rather',\n",
       " 'another',\n",
       " 'name',\n",
       " 'for',\n",
       " 'statistics',\n",
       " '.',\n",
       " '16',\n",
       " 'Others',\n",
       " 'argue',\n",
       " 'that',\n",
       " 'data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'distinct',\n",
       " 'from',\n",
       " 'statistics',\n",
       " 'because',\n",
       " 'it',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'problems',\n",
       " 'and',\n",
       " 'techniques',\n",
       " 'unique',\n",
       " 'to',\n",
       " 'digital',\n",
       " 'data',\n",
       " '.',\n",
       " '17',\n",
       " 'Vasant',\n",
       " 'Dhar',\n",
       " 'writes',\n",
       " 'that',\n",
       " 'statistics',\n",
       " 'emphasizes',\n",
       " 'quantitative',\n",
       " 'data',\n",
       " 'and',\n",
       " 'description',\n",
       " '.',\n",
       " 'In',\n",
       " 'contrast',\n",
       " 'data',\n",
       " 'science',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'quantitative',\n",
       " 'and',\n",
       " 'qualitative',\n",
       " 'data',\n",
       " 'e',\n",
       " '.',\n",
       " 'g',\n",
       " '.',\n",
       " 'from',\n",
       " 'images',\n",
       " 'text',\n",
       " 'sensors',\n",
       " 'transactions',\n",
       " 'customer',\n",
       " 'information',\n",
       " 'etc',\n",
       " '.',\n",
       " 'and',\n",
       " 'emphasizes',\n",
       " 'prediction',\n",
       " 'and',\n",
       " 'action',\n",
       " '.',\n",
       " '18',\n",
       " 'Andrew',\n",
       " 'Gelman',\n",
       " 'of',\n",
       " 'Columbia',\n",
       " 'University',\n",
       " 'has',\n",
       " 'described',\n",
       " 'statistics',\n",
       " 'as',\n",
       " 'a',\n",
       " 'non',\n",
       " 'essential',\n",
       " 'part',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " '.',\n",
       " '19',\n",
       " 'Stanford',\n",
       " 'professor',\n",
       " 'David',\n",
       " 'Donoho',\n",
       " 'writes',\n",
       " 'that',\n",
       " 'data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'not',\n",
       " 'distinguished',\n",
       " 'from',\n",
       " 'statistics',\n",
       " 'by',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'datasets',\n",
       " 'or',\n",
       " 'use',\n",
       " 'of',\n",
       " 'computing',\n",
       " 'and',\n",
       " 'that',\n",
       " 'many',\n",
       " 'graduate',\n",
       " 'programs',\n",
       " 'misleadingly',\n",
       " 'advertise',\n",
       " 'their',\n",
       " 'analytics',\n",
       " 'and',\n",
       " 'statistics',\n",
       " 'training',\n",
       " 'as',\n",
       " 'the',\n",
       " 'essence',\n",
       " 'of',\n",
       " 'a',\n",
       " 'data',\n",
       " 'science',\n",
       " 'program',\n",
       " '.',\n",
       " 'He',\n",
       " 'describes',\n",
       " 'data',\n",
       " 'science',\n",
       " 'as',\n",
       " 'an',\n",
       " 'applied',\n",
       " 'field',\n",
       " 'growing',\n",
       " 'out',\n",
       " 'of',\n",
       " 'traditional',\n",
       " 'statistics',\n",
       " '.',\n",
       " '20',\n",
       " 'In',\n",
       " '1962',\n",
       " 'John',\n",
       " 'Tukey',\n",
       " 'described',\n",
       " 'a',\n",
       " 'field',\n",
       " 'he',\n",
       " 'called',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'which',\n",
       " 'resembles',\n",
       " 'modern',\n",
       " 'data',\n",
       " 'science',\n",
       " '.',\n",
       " '20',\n",
       " 'In',\n",
       " '1985',\n",
       " 'in',\n",
       " 'a',\n",
       " 'lecture',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Chinese',\n",
       " 'Academy',\n",
       " 'of',\n",
       " 'Sciences',\n",
       " 'in',\n",
       " 'Beijing',\n",
       " 'C',\n",
       " '.',\n",
       " 'F',\n",
       " '.',\n",
       " 'Jeff',\n",
       " 'Wu',\n",
       " 'used',\n",
       " 'the',\n",
       " 'term',\n",
       " 'data',\n",
       " 'science',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'as',\n",
       " 'an',\n",
       " 'alternative',\n",
       " 'name',\n",
       " 'for',\n",
       " 'statistics',\n",
       " '.',\n",
       " '21',\n",
       " 'Later',\n",
       " 'attendees',\n",
       " 'at',\n",
       " 'a',\n",
       " '1992',\n",
       " 'statistics',\n",
       " 'symposium',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Montpellier',\n",
       " 'II',\n",
       " 'acknowledged',\n",
       " 'the',\n",
       " 'emergence',\n",
       " 'of',\n",
       " 'a',\n",
       " 'new',\n",
       " 'discipline',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'data',\n",
       " 'of',\n",
       " 'various',\n",
       " 'origins',\n",
       " 'and',\n",
       " 'forms',\n",
       " 'combining',\n",
       " 'established',\n",
       " 'concepts',\n",
       " 'and',\n",
       " 'principles',\n",
       " 'of',\n",
       " 'statistics',\n",
       " 'and',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'with',\n",
       " 'computing',\n",
       " '.',\n",
       " '22',\n",
       " '23',\n",
       " 'The',\n",
       " 'term',\n",
       " 'data',\n",
       " 'science',\n",
       " 'has',\n",
       " 'been',\n",
       " 'traced',\n",
       " 'back',\n",
       " 'to',\n",
       " '1974',\n",
       " 'when',\n",
       " 'Peter',\n",
       " 'Naur',\n",
       " 'proposed',\n",
       " 'it',\n",
       " 'as',\n",
       " 'an',\n",
       " 'alternative',\n",
       " 'name',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'science',\n",
       " '.',\n",
       " '6',\n",
       " 'In',\n",
       " '1996',\n",
       " 'the',\n",
       " 'International',\n",
       " 'Federation',\n",
       " 'of',\n",
       " 'Classification',\n",
       " 'Societies',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'conference',\n",
       " 'to',\n",
       " 'specifically',\n",
       " 'feature',\n",
       " 'data',\n",
       " 'science',\n",
       " 'as',\n",
       " 'a',\n",
       " 'topic',\n",
       " '.',\n",
       " '6',\n",
       " 'However',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'was',\n",
       " 'still',\n",
       " 'in',\n",
       " 'flux',\n",
       " '.',\n",
       " 'After',\n",
       " 'the',\n",
       " '1985',\n",
       " 'lecture',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Chinese',\n",
       " 'Academy',\n",
       " 'of',\n",
       " 'Sciences',\n",
       " 'in',\n",
       " 'Beijing',\n",
       " 'in',\n",
       " '1997',\n",
       " 'C',\n",
       " '.',\n",
       " 'F',\n",
       " '.',\n",
       " 'Jeff',\n",
       " 'Wu',\n",
       " 'again',\n",
       " 'suggested',\n",
       " 'that',\n",
       " 'statistics',\n",
       " 'should',\n",
       " 'be',\n",
       " 'renamed',\n",
       " 'data',\n",
       " 'science',\n",
       " '.',\n",
       " 'He',\n",
       " 'reasoned',\n",
       " 'that',\n",
       " 'a',\n",
       " 'new',\n",
       " 'name',\n",
       " 'would',\n",
       " 'help',\n",
       " 'statistics',\n",
       " 'shed',\n",
       " 'inaccurate',\n",
       " 'stereotypes',\n",
       " 'such',\n",
       " 'as',\n",
       " 'being',\n",
       " 'synonymous',\n",
       " 'with',\n",
       " 'accounting',\n",
       " 'or',\n",
       " 'limited',\n",
       " 'to',\n",
       " 'describing',\n",
       " 'data',\n",
       " '.',\n",
       " '24',\n",
       " 'In',\n",
       " '1998',\n",
       " 'Hayashi',\n",
       " 'Chikio',\n",
       " 'argued',\n",
       " 'for',\n",
       " 'data',\n",
       " 'science',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'interdisciplinary',\n",
       " 'concept',\n",
       " 'with',\n",
       " 'three',\n",
       " 'aspects',\n",
       " 'data',\n",
       " 'design',\n",
       " 'collection',\n",
       " 'and',\n",
       " 'analysis',\n",
       " '.',\n",
       " '23',\n",
       " 'During',\n",
       " 'the',\n",
       " '1990s',\n",
       " 'popular',\n",
       " 'terms',\n",
       " 'for',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'patterns',\n",
       " 'in',\n",
       " 'datasets',\n",
       " 'which',\n",
       " 'were',\n",
       " 'increasingly',\n",
       " 'large',\n",
       " 'included',\n",
       " 'knowledge',\n",
       " 'discovery',\n",
       " 'and',\n",
       " 'data',\n",
       " 'mining',\n",
       " '.',\n",
       " '6',\n",
       " '25',\n",
       " 'In',\n",
       " '2012',\n",
       " 'technologists',\n",
       " 'Thomas',\n",
       " 'H',\n",
       " '.',\n",
       " 'Davenport',\n",
       " 'and',\n",
       " 'DJ',\n",
       " 'Patil',\n",
       " 'declared',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " 'The',\n",
       " 'Sexiest',\n",
       " 'Job',\n",
       " 'of',\n",
       " 'the',\n",
       " '21st',\n",
       " 'Century',\n",
       " '26',\n",
       " 'a',\n",
       " 'catchphrase',\n",
       " 'that',\n",
       " 'was',\n",
       " 'picked',\n",
       " 'up',\n",
       " 'even',\n",
       " 'by',\n",
       " 'major',\n",
       " 'city',\n",
       " 'newspapers',\n",
       " 'like',\n",
       " 'the',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " '27',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Boston',\n",
       " 'Globe',\n",
       " '.',\n",
       " '28',\n",
       " 'A',\n",
       " 'decade',\n",
       " 'later',\n",
       " 'they',\n",
       " 'reaffirmed',\n",
       " 'it',\n",
       " 'stating',\n",
       " 'that',\n",
       " 'the',\n",
       " 'job',\n",
       " 'is',\n",
       " 'more',\n",
       " 'in',\n",
       " 'demand',\n",
       " 'than',\n",
       " 'ever',\n",
       " 'with',\n",
       " 'employers',\n",
       " '.',\n",
       " '29',\n",
       " 'The',\n",
       " 'modern',\n",
       " 'conception',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " 'as',\n",
       " 'an',\n",
       " 'independent',\n",
       " 'discipline',\n",
       " 'is',\n",
       " 'sometimes',\n",
       " 'attributed',\n",
       " 'to',\n",
       " 'William',\n",
       " 'S',\n",
       " '.',\n",
       " 'Cleveland',\n",
       " '.',\n",
       " '30',\n",
       " 'In',\n",
       " 'a',\n",
       " '2001',\n",
       " 'paper',\n",
       " 'he',\n",
       " 'advocated',\n",
       " 'an',\n",
       " 'expansion',\n",
       " 'of',\n",
       " 'statistics',\n",
       " 'beyond',\n",
       " 'theory',\n",
       " 'into',\n",
       " 'technical',\n",
       " 'areas',\n",
       " 'because',\n",
       " 'this',\n",
       " 'would',\n",
       " 'significantly',\n",
       " 'change',\n",
       " 'the',\n",
       " 'field',\n",
       " 'it',\n",
       " 'warranted',\n",
       " 'a',\n",
       " 'new',\n",
       " 'name',\n",
       " '.',\n",
       " '25',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'became',\n",
       " 'more',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'few',\n",
       " 'years',\n",
       " 'in',\n",
       " '2002',\n",
       " 'the',\n",
       " 'Committee',\n",
       " 'on',\n",
       " 'Data',\n",
       " 'for',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Technology',\n",
       " 'launched',\n",
       " 'the',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Journal',\n",
       " '.',\n",
       " 'In',\n",
       " '2003',\n",
       " 'Columbia',\n",
       " 'University',\n",
       " 'launched',\n",
       " 'The',\n",
       " 'Journal',\n",
       " 'of',\n",
       " 'Data',\n",
       " 'Science',\n",
       " '.',\n",
       " '25',\n",
       " 'In',\n",
       " '2014',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Statistical',\n",
       " \"Association's\",\n",
       " 'Section',\n",
       " 'on',\n",
       " 'Statistical',\n",
       " 'Learning',\n",
       " 'and',\n",
       " 'Data',\n",
       " 'Mining',\n",
       " 'changed',\n",
       " 'its',\n",
       " 'name',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Section',\n",
       " 'on',\n",
       " 'Statistical',\n",
       " 'Learning',\n",
       " 'and',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'reflecting',\n",
       " 'the',\n",
       " 'ascendant',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " '.',\n",
       " '31',\n",
       " 'The',\n",
       " 'professional',\n",
       " 'title',\n",
       " 'of',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'has',\n",
       " 'been',\n",
       " 'attributed',\n",
       " 'to',\n",
       " 'DJ',\n",
       " 'Patil',\n",
       " 'and',\n",
       " 'Jeff',\n",
       " 'Hammerbacher',\n",
       " 'in',\n",
       " '2008',\n",
       " '.',\n",
       " '32',\n",
       " 'Though',\n",
       " 'it',\n",
       " 'was',\n",
       " 'used',\n",
       " 'by',\n",
       " 'the',\n",
       " 'National',\n",
       " 'Science',\n",
       " 'Board',\n",
       " 'in',\n",
       " 'their',\n",
       " '2005',\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Actuarial_science\"\n",
    "# url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "# content = soup.find(\"div\", \"bodyContent\")\n",
    "content = soup.find(\"div\", \"mw-content-ltr\")\n",
    "regex = r\"[\\w']+|[\\.]\"\n",
    "\n",
    "document = []\n",
    "for paragraph in content(\"p\"):\n",
    "  words = re.findall(regex, paragraph.text)\n",
    "  document.extend(words)\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science is an interdisciplinary academic field 1 that uses statistics scientific computing scientific methods processes algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy structured or unstructured data.',\n",
       " '2 Data science also integrates domain knowledge from the underlying application domain e.',\n",
       " 'g.',\n",
       " 'natural sciences information technology and medicine.',\n",
       " '3 Data science is multifaceted and can be described as a science a research paradigm a research method a discipline a workflow and a profession.',\n",
       " '4 Data science is a concept to unify statistics data analysis informatics and their related methods to understand and analyze actual phenomena with data.',\n",
       " '5 It uses techniques and theories drawn from many fields within the context of mathematics statistics computer science information science and domain knowledge.',\n",
       " '6 However data science is different from computer science and information science.',\n",
       " 'Turing Award winner Jim Gray imagined data science as a fourth paradigm of science empirical theoretical computational and now data driven and asserted that everything about science is changing because of the impact of information technology and the data deluge.',\n",
       " '7 8 A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.',\n",
       " '9 Data science is an interdisciplinary field 10 focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains.',\n",
       " 'The field encompasses preparing data for analysis formulating data science problems analyzing data developing data driven solutions and presenting findings to inform high level decisions in a broad range of application domains.',\n",
       " 'As such it incorporates skills from computer science statistics information science mathematics data visualization information visualization data sonification data integration graphic design complex systems communication and business.',\n",
       " '11 12 Statistician Nathan Yau drawing on Ben Fry also links data science to human computer interaction users should be able to intuitively control and explore data.',\n",
       " '13 14 In 2015 the American Statistical Association identified database management statistics and machine learning and distributed and parallel systems as the three emerging foundational professional communities.',\n",
       " '15 Many statisticians including Nate Silver have argued that data science is not a new field but rather another name for statistics.',\n",
       " '16 Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.',\n",
       " '17 Vasant Dhar writes that statistics emphasizes quantitative data and description.',\n",
       " 'In contrast data science deals with quantitative and qualitative data e.',\n",
       " 'g.',\n",
       " 'from images text sensors transactions customer information etc.',\n",
       " 'and emphasizes prediction and action.',\n",
       " '18 Andrew Gelman of Columbia University has described statistics as a non essential part of data science.',\n",
       " '19 Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program.',\n",
       " 'He describes data science as an applied field growing out of traditional statistics.',\n",
       " '20 In 1962 John Tukey described a field he called data analysis which resembles modern data science.',\n",
       " '20 In 1985 in a lecture given to the Chinese Academy of Sciences in Beijing C.',\n",
       " 'F.',\n",
       " 'Jeff Wu used the term data science for the first time as an alternative name for statistics.',\n",
       " '21 Later attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms combining established concepts and principles of statistics and data analysis with computing.',\n",
       " '22 23 The term data science has been traced back to 1974 when Peter Naur proposed it as an alternative name to computer science.',\n",
       " '6 In 1996 the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.',\n",
       " '6 However the definition was still in flux.',\n",
       " 'After the 1985 lecture at the Chinese Academy of Sciences in Beijing in 1997 C.',\n",
       " 'F.',\n",
       " 'Jeff Wu again suggested that statistics should be renamed data science.',\n",
       " 'He reasoned that a new name would help statistics shed inaccurate stereotypes such as being synonymous with accounting or limited to describing data.',\n",
       " '24 In 1998 Hayashi Chikio argued for data science as a new interdisciplinary concept with three aspects data design collection and analysis.',\n",
       " '23 During the 1990s popular terms for the process of finding patterns in datasets which were increasingly large included knowledge discovery and data mining.',\n",
       " '6 25 In 2012 technologists Thomas H.',\n",
       " 'Davenport and DJ Patil declared Data Scientist The Sexiest Job of the 21st Century 26 a catchphrase that was picked up even by major city newspapers like the New York Times 27 and the Boston Globe.',\n",
       " '28 A decade later they reaffirmed it stating that the job is more in demand than ever with employers.',\n",
       " '29 The modern conception of data science as an independent discipline is sometimes attributed to William S.',\n",
       " 'Cleveland.',\n",
       " '30 In a 2001 paper he advocated an expansion of statistics beyond theory into technical areas because this would significantly change the field it warranted a new name.',\n",
       " '25 Data science became more widely used in the next few years in 2002 the Committee on Data for Science and Technology launched the Data Science Journal.',\n",
       " 'In 2003 Columbia University launched The Journal of Data Science.',\n",
       " \"25 In 2014 the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science reflecting the ascendant popularity of data science.\",\n",
       " '31 The professional title of data scientist has been attributed to DJ Patil and Jeff Hammerbacher in 2008.',\n",
       " '32 Though it was used by the National Science Board in their 2005 report Long Lived Digital Data Collections Enabling Research and Education in the 21st Century it referred broadly to any key role in managing a digital data collection.',\n",
       " '33 There is still no consensus on the definition of data science and it is considered by some to be a buzzword.',\n",
       " '34 Big data is a related marketing term.',\n",
       " '35 Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.',\n",
       " '36 Data science and data analysis are both important disciplines in the field of data management and analysis but they differ in several key ways.',\n",
       " 'While both fields involve working with data data science is more of an interdisciplinary field that involves the application of statistical computational and machine learning methods to extract insights from data and make predictions while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.',\n",
       " '37 38 Data analysis typically involves working with smaller structured datasets to answer specific questions or solve specific problems.',\n",
       " 'This can involve tasks such as data cleaning data visualization and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables.',\n",
       " 'Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data.',\n",
       " 'For example a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.',\n",
       " '37 Data science on the other hand is a more complex and iterative process that involves working with larger more complex datasets that often require advanced computational and statistical methods to analyze.',\n",
       " 'Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data driven decisions.',\n",
       " 'In addition to statistical analysis data science often involves tasks such as data preprocessing feature engineering and model selection.',\n",
       " 'For instance a data scientist might develop a recommendation system for an e commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.',\n",
       " '38 39 While data analysis focuses on extracting insights from existing data data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions.',\n",
       " 'Data scientists are often responsible for collecting and cleaning data selecting appropriate analytical techniques and deploying models in real world scenarios.',\n",
       " 'They work at the intersection of mathematics computer science and domain expertise to solve complex problems and uncover hidden patterns in large datasets.',\n",
       " '38 Despite these differences data science and data analysis are closely related fields and often require similar skill sets.',\n",
       " 'Both fields require a solid foundation in statistics programming and data visualization as well as the ability to communicate findings effectively to both technical and non technical audiences.',\n",
       " 'Both fields benefit from critical thinking and domain knowledge as understanding the context and nuances of the data is essential for accurate analysis and modeling.',\n",
       " '37 38 In summary data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis.',\n",
       " 'Data analysis focuses on extracting insights and drawing conclusions from structured data while data science involves a more comprehensive approach that combines statistical analysis computational methods and machine learning to extract insights build predictive models and drive data driven decision making.',\n",
       " 'Both fields use data to understand patterns make informed decisions and solve complex problems across various domains.',\n",
       " 'Cloud computing can offer access to large amounts of computational power and storage.',\n",
       " '40 In big data where volumes of information are continually generated and processed these platforms can be used to handle complex and resource intensive analytical tasks.',\n",
       " '41 Some distributed computing frameworks are designed to handle big data workloads.',\n",
       " 'These frameworks can enable data scientists to process and analyze large datasets in parallel which can reducing processing times.',\n",
       " '42 Data science involve collecting processing and analyzing data which often including personal and sensitive information.',\n",
       " 'Ethical concerns include potential privacy violations bias perpetuation and negative societal impacts 43 44 Machine learning models can amplify existing biases present in training data leading to discriminatory or unfair outcomes.',\n",
       " '45 46.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform document into sentence\n",
    "sentences = [sentence.strip()+\".\" for sentence in \" \".join(document).split(\".\")]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = \"\\n\".join(sentences)\n",
    "with open(\"./datasets/wiki_article.txt\", \"w\") as fname:\n",
    "  fname.write(corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing training data for word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the non empty lines to construct a `tf.data.TextLineDataset` object for  \n",
    "the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FilterDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_to_article = \"./datasets/wiki_article.txt\"\n",
    "path_to_article = \"./datasets/wiki_article_clean.txt\"\n",
    "# path_to_article = \"./datasets/wiki_two_article_clean.txt\"\n",
    "text_ds = tf.data.TextLineDataset(path_to_article).filter(\n",
    "  lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create custom standardization function to lowercase the text and\n",
    "# remove punctuation\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence\n",
    "vocab_size = 4096  # default for large corpus 4096\n",
    "sequence_length = 30 \n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "  standardize=custom_standardization, \n",
    "  max_tokens=vocab_size,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `TextVectorization.adapt` on the text dataset to create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'of', 'and', 'to', 'in', 'actuarial', 'a', 'insurance', 'as', 'for', 'science', 'is', 'financial', 'by', 'life', 'be', 'actuaries', 'models']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vectorize_layer` can now be used to generate vectors for each element in   \n",
    "the `text_ds` (a `tf.data.Dataset`). Apply `Dataset.batch`, `Dataset.prefetch`,  \n",
    "`Dataset.map`, and `Dataset.unbatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sequence` (singular) represents each sentence (or row) in `wiki_article.txt`.  \n",
    "`sequences` (plural) is now a list of int encoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect a few examples from `sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7  12  13   2  76  20 717  34   4 141  30   5  82  32   6   9  33  72\n",
      " 184   4  39 499   4 383   0   0   0   0   0   0] => ['actuarial', 'science', 'is', 'the', 'discipline', 'that', 'applies', 'mathematical', 'and', 'statistical', 'methods', 'to', 'assess', 'risk', 'in', 'insurance', 'pension', 'finance', 'investment', 'and', 'other', 'industries', 'and', 'professions', '', '', '', '', '', '']\n",
      "[ 65 193  18 238 151 102   5 457 467   3 135   4  16 565   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] => ['more', 'generally', 'actuaries', 'apply', 'rigorous', 'mathematics', 'to', 'model', 'matters', 'of', 'uncertainty', 'and', 'life', 'expectancy', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[ 18  44 384 283   6  28  76   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] => ['actuaries', 'are', 'professionals', 'trained', 'in', 'this', 'discipline', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[  6  35 633  18 448 213  59 651  15 416   8 330   3 151 385 569 548   6\n",
      " 112  24  10  96   4 393 722   0   0   0   0   0] => ['in', 'many', 'countries', 'actuaries', 'must', 'demonstrate', 'their', 'competence', 'by', 'passing', 'a', 'series', 'of', 'rigorous', 'professional', 'examinations', 'focused', 'in', 'fields', 'such', 'as', 'probability', 'and', 'predictive', 'analysis', '', '', '', '', '']\n",
      "[  7  12 509   8 439   3 493 306 508 102  96  23 312  72  36  14 243   4\n",
      " 221  12   0   0   0   0   0   0   0   0   0   0] => ['actuarial', 'science', 'includes', 'a', 'number', 'of', 'interrelated', 'subjects', 'including', 'mathematics', 'probability', 'theory', 'statistics', 'finance', 'economics', 'financial', 'accounting', 'and', 'computer', 'science', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate training examples from sequences.   \n",
    "This function iterates over each word from each sequence to collect positive   \n",
    "and negative context words. Length of target, contexts and labels should be  \n",
    "the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence). \n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      sequence, vocabulary_size=vocab_size, sampling_table=sampling_table,\n",
    "      window_size=window_size, negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce trainig examples \n",
    "    # with a positive context word and negative samples\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "        true_classes=context_class, num_true=1, num_sampled=num_ns,\n",
    "        unique=True, range_max=vocab_size, seed=seed, name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for on target word)\n",
    "      context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:00<00:00, 980.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tagets.shape: (538,)\n",
      "contexts.shape: (538, 5)\n",
      "labels.shape: (538, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 24_04_28 \n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "  sequences=sequences, window_size=2, num_ns=4, vocab_size=vocab_size, \n",
    "  seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"tagets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform efficient batching for the potentially large number of training  \n",
    "examples, use the `tf.data.Dataset` API.\n",
    "\n",
    "And also apply `Dataset.cache` and `Dataset.prefetch` to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(16,), dtype=tf.int64, name=None), TensorSpec(shape=(16, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(16, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16   # Set it less than number of samples\n",
    "BUFFER_SIZE = 10\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(\n",
    "      vocab_size, embedding_dim, name=\"w2v_embedding\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(\n",
    "      vocab_size, embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)     # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    #target: (batch, )\n",
    "\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb = (batch, context, embed)\n",
    "\n",
    "    # b: batch index\n",
    "    # e: embedding index\n",
    "    # c: context index\n",
    "    dots = tf.einsum(\"be,bce->bc\", word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "  return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10   # default 128\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(\n",
    "  optimizer=\"adam\", \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the `dataset` for some number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "33/33 [==============================] - 0s 719us/step - loss: 1.2593 - accuracy: 0.9167\n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 1.2257 - accuracy: 0.9148\n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 0s 735us/step - loss: 1.1916 - accuracy: 0.9148\n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 0s 813us/step - loss: 1.1573 - accuracy: 0.9167\n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 1.1229 - accuracy: 0.9167\n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 1.0886 - accuracy: 0.9223\n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 0s 731us/step - loss: 1.0545 - accuracy: 0.9223\n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 1.0208 - accuracy: 0.9242\n",
      "Epoch 9/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.9875 - accuracy: 0.9242\n",
      "Epoch 10/200\n",
      "33/33 [==============================] - 0s 735us/step - loss: 0.9548 - accuracy: 0.9242\n",
      "Epoch 11/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.9227 - accuracy: 0.9261\n",
      "Epoch 12/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.8913 - accuracy: 0.9299\n",
      "Epoch 13/200\n",
      "33/33 [==============================] - 0s 725us/step - loss: 0.8607 - accuracy: 0.9318\n",
      "Epoch 14/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.8309 - accuracy: 0.9318\n",
      "Epoch 15/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.8020 - accuracy: 0.9356\n",
      "Epoch 16/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.7739 - accuracy: 0.9356\n",
      "Epoch 17/200\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.7467 - accuracy: 0.9375\n",
      "Epoch 18/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.7205 - accuracy: 0.9375\n",
      "Epoch 19/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.6951 - accuracy: 0.9375\n",
      "Epoch 20/200\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.6706 - accuracy: 0.9432\n",
      "Epoch 21/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.6470 - accuracy: 0.9451\n",
      "Epoch 22/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.6243 - accuracy: 0.9451\n",
      "Epoch 23/200\n",
      "33/33 [==============================] - 0s 737us/step - loss: 0.6025 - accuracy: 0.9451\n",
      "Epoch 24/200\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.9508\n",
      "Epoch 25/200\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.5612 - accuracy: 0.9564\n",
      "Epoch 26/200\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.5418 - accuracy: 0.9583\n",
      "Epoch 27/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.5231 - accuracy: 0.9602\n",
      "Epoch 28/200\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.5052 - accuracy: 0.9602\n",
      "Epoch 29/200\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.4880 - accuracy: 0.9659\n",
      "Epoch 30/200\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.4715 - accuracy: 0.9659\n",
      "Epoch 31/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.4556 - accuracy: 0.9659\n",
      "Epoch 32/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.4404 - accuracy: 0.9678\n",
      "Epoch 33/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.4258 - accuracy: 0.9678\n",
      "Epoch 34/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.4117 - accuracy: 0.9678\n",
      "Epoch 35/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.3983 - accuracy: 0.9678\n",
      "Epoch 36/200\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.3854 - accuracy: 0.9716\n",
      "Epoch 37/200\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.3729 - accuracy: 0.9735\n",
      "Epoch 38/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3610 - accuracy: 0.9735\n",
      "Epoch 39/200\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.3496 - accuracy: 0.9754\n",
      "Epoch 40/200\n",
      "33/33 [==============================] - 0s 719us/step - loss: 0.3386 - accuracy: 0.9773\n",
      "Epoch 41/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.3280 - accuracy: 0.9773\n",
      "Epoch 42/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.3179 - accuracy: 0.9773\n",
      "Epoch 43/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3081 - accuracy: 0.9773\n",
      "Epoch 44/200\n",
      "33/33 [==============================] - 0s 719us/step - loss: 0.2987 - accuracy: 0.9811\n",
      "Epoch 45/200\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.2897 - accuracy: 0.9830\n",
      "Epoch 46/200\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.2811 - accuracy: 0.9830\n",
      "Epoch 47/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.2727 - accuracy: 0.9830\n",
      "Epoch 48/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.2647 - accuracy: 0.9830\n",
      "Epoch 49/200\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.2570 - accuracy: 0.9830\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.2496 - accuracy: 0.9830\n",
      "Epoch 51/200\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9830\n",
      "Epoch 52/200\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.2355 - accuracy: 0.9830\n",
      "Epoch 53/200\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.2289 - accuracy: 0.9848\n",
      "Epoch 54/200\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2225 - accuracy: 0.9867\n",
      "Epoch 55/200\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2164 - accuracy: 0.9867\n",
      "Epoch 56/200\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.2104 - accuracy: 0.9886\n",
      "Epoch 57/200\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2047 - accuracy: 0.9905\n",
      "Epoch 58/200\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.1992 - accuracy: 0.9905\n",
      "Epoch 59/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.1939 - accuracy: 0.9924\n",
      "Epoch 60/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.1888 - accuracy: 0.9924\n",
      "Epoch 61/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.1838 - accuracy: 0.9943\n",
      "Epoch 62/200\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1791 - accuracy: 0.9943\n",
      "Epoch 63/200\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.1745 - accuracy: 0.9943\n",
      "Epoch 64/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.1700 - accuracy: 0.9962\n",
      "Epoch 65/200\n",
      "33/33 [==============================] - 0s 897us/step - loss: 0.1657 - accuracy: 0.9962\n",
      "Epoch 66/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.1616 - accuracy: 0.9962\n",
      "Epoch 67/200\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.1576 - accuracy: 0.9962\n",
      "Epoch 68/200\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.1537 - accuracy: 0.9962\n",
      "Epoch 69/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1500 - accuracy: 0.9962\n",
      "Epoch 70/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1464 - accuracy: 0.9962\n",
      "Epoch 71/200\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.1429 - accuracy: 0.9962\n",
      "Epoch 72/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1396 - accuracy: 0.9962\n",
      "Epoch 73/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.1363 - accuracy: 0.9962\n",
      "Epoch 74/200\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.1332 - accuracy: 0.9962\n",
      "Epoch 75/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.1301 - accuracy: 0.9962\n",
      "Epoch 76/200\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.1272 - accuracy: 0.9962\n",
      "Epoch 77/200\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.1244 - accuracy: 0.9962\n",
      "Epoch 78/200\n",
      "33/33 [==============================] - 0s 956us/step - loss: 0.1216 - accuracy: 0.9962\n",
      "Epoch 79/200\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.1189 - accuracy: 0.9962\n",
      "Epoch 80/200\n",
      "33/33 [==============================] - 0s 788us/step - loss: 0.1164 - accuracy: 0.9962\n",
      "Epoch 81/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.1139 - accuracy: 0.9962\n",
      "Epoch 82/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.1115 - accuracy: 0.9981\n",
      "Epoch 83/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.1091 - accuracy: 0.9981\n",
      "Epoch 84/200\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.1069 - accuracy: 0.9981\n",
      "Epoch 85/200\n",
      "33/33 [==============================] - 0s 719us/step - loss: 0.1047 - accuracy: 0.9981\n",
      "Epoch 86/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.1025 - accuracy: 0.9981\n",
      "Epoch 87/200\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.1005 - accuracy: 0.9981\n",
      "Epoch 88/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0985 - accuracy: 0.9981\n",
      "Epoch 89/200\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0966 - accuracy: 0.9981\n",
      "Epoch 90/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0947 - accuracy: 0.9981\n",
      "Epoch 91/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0929 - accuracy: 0.9981\n",
      "Epoch 92/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0911 - accuracy: 0.9981\n",
      "Epoch 93/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0894 - accuracy: 0.9981\n",
      "Epoch 94/200\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.0878 - accuracy: 0.9981\n",
      "Epoch 95/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0862 - accuracy: 0.9981\n",
      "Epoch 96/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0846 - accuracy: 0.9981\n",
      "Epoch 97/200\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.0831 - accuracy: 0.9981\n",
      "Epoch 98/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.0816 - accuracy: 0.9981\n",
      "Epoch 99/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0802 - accuracy: 0.9981\n",
      "Epoch 100/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0789 - accuracy: 0.9981\n",
      "Epoch 101/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0775 - accuracy: 0.9981\n",
      "Epoch 102/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0762 - accuracy: 0.9981\n",
      "Epoch 103/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0750 - accuracy: 0.9962\n",
      "Epoch 104/200\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.0737 - accuracy: 0.9962\n",
      "Epoch 105/200\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.0725 - accuracy: 0.9962\n",
      "Epoch 106/200\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9962\n",
      "Epoch 107/200\n",
      "33/33 [==============================] - 0s 822us/step - loss: 0.0703 - accuracy: 0.9962\n",
      "Epoch 108/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.0692 - accuracy: 0.9962\n",
      "Epoch 109/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0681 - accuracy: 0.9962\n",
      "Epoch 110/200\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.0671 - accuracy: 0.9962\n",
      "Epoch 111/200\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.0661 - accuracy: 0.9962\n",
      "Epoch 112/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0651 - accuracy: 0.9962\n",
      "Epoch 113/200\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.0642 - accuracy: 0.9962\n",
      "Epoch 114/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0633 - accuracy: 0.9962\n",
      "Epoch 115/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0624 - accuracy: 0.9962\n",
      "Epoch 116/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0615 - accuracy: 0.9962\n",
      "Epoch 117/200\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.0606 - accuracy: 0.9962\n",
      "Epoch 118/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0598 - accuracy: 0.9962\n",
      "Epoch 119/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0590 - accuracy: 0.9962\n",
      "Epoch 120/200\n",
      "33/33 [==============================] - 0s 736us/step - loss: 0.0582 - accuracy: 0.9962\n",
      "Epoch 121/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0575 - accuracy: 0.9962\n",
      "Epoch 122/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.0568 - accuracy: 0.9962\n",
      "Epoch 123/200\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.0560 - accuracy: 0.9962\n",
      "Epoch 124/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0553 - accuracy: 0.9962\n",
      "Epoch 125/200\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.0547 - accuracy: 0.9962\n",
      "Epoch 126/200\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.0540 - accuracy: 0.9962\n",
      "Epoch 127/200\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.0534 - accuracy: 0.9962\n",
      "Epoch 128/200\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.0527 - accuracy: 0.9962\n",
      "Epoch 129/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0521 - accuracy: 0.9962\n",
      "Epoch 130/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0515 - accuracy: 0.9962\n",
      "Epoch 131/200\n",
      "33/33 [==============================] - 0s 908us/step - loss: 0.0509 - accuracy: 0.9962\n",
      "Epoch 132/200\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0504 - accuracy: 0.9962\n",
      "Epoch 133/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0498 - accuracy: 0.9962\n",
      "Epoch 134/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0493 - accuracy: 0.9962\n",
      "Epoch 135/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0488 - accuracy: 0.9962\n",
      "Epoch 136/200\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.0483 - accuracy: 0.9962\n",
      "Epoch 137/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0478 - accuracy: 0.9962\n",
      "Epoch 138/200\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.0473 - accuracy: 0.9962\n",
      "Epoch 139/200\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.0468 - accuracy: 0.9962\n",
      "Epoch 140/200\n",
      "33/33 [==============================] - 0s 719us/step - loss: 0.0463 - accuracy: 0.9962\n",
      "Epoch 141/200\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0459 - accuracy: 0.9962\n",
      "Epoch 142/200\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.0455 - accuracy: 0.9962\n",
      "Epoch 143/200\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.0450 - accuracy: 0.9962\n",
      "Epoch 144/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0446 - accuracy: 0.9962\n",
      "Epoch 145/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0442 - accuracy: 0.9962\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0438 - accuracy: 0.9962\n",
      "Epoch 147/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0434 - accuracy: 0.9962\n",
      "Epoch 148/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0430 - accuracy: 0.9962\n",
      "Epoch 149/200\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.0427 - accuracy: 0.9962\n",
      "Epoch 150/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0423 - accuracy: 0.9962\n",
      "Epoch 151/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0420 - accuracy: 0.9962\n",
      "Epoch 152/200\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.0416 - accuracy: 0.9962\n",
      "Epoch 153/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0413 - accuracy: 0.9962\n",
      "Epoch 154/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0410 - accuracy: 0.9962\n",
      "Epoch 155/200\n",
      "33/33 [==============================] - 0s 896us/step - loss: 0.0407 - accuracy: 0.9962\n",
      "Epoch 156/200\n",
      "33/33 [==============================] - 0s 907us/step - loss: 0.0403 - accuracy: 0.9962\n",
      "Epoch 157/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0400 - accuracy: 0.9962\n",
      "Epoch 158/200\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.0397 - accuracy: 0.9962\n",
      "Epoch 159/200\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.0395 - accuracy: 0.9962\n",
      "Epoch 160/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0392 - accuracy: 0.9962\n",
      "Epoch 161/200\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.0389 - accuracy: 0.9962\n",
      "Epoch 162/200\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.0386 - accuracy: 0.9962\n",
      "Epoch 163/200\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0384 - accuracy: 0.9962\n",
      "Epoch 164/200\n",
      "33/33 [==============================] - 0s 847us/step - loss: 0.0381 - accuracy: 0.9962\n",
      "Epoch 165/200\n",
      "33/33 [==============================] - 0s 831us/step - loss: 0.0379 - accuracy: 0.9962\n",
      "Epoch 166/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0376 - accuracy: 0.9962\n",
      "Epoch 167/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0374 - accuracy: 0.9962\n",
      "Epoch 168/200\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.0371 - accuracy: 0.9962\n",
      "Epoch 169/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.0369 - accuracy: 0.9962\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.0367 - accuracy: 0.9962\n",
      "Epoch 171/200\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.0365 - accuracy: 0.9962\n",
      "Epoch 172/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0363 - accuracy: 0.9962\n",
      "Epoch 173/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0360 - accuracy: 0.9962\n",
      "Epoch 174/200\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.0358 - accuracy: 0.9962\n",
      "Epoch 175/200\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.0356 - accuracy: 0.9962\n",
      "Epoch 176/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0355 - accuracy: 0.9962\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0353 - accuracy: 0.9962\n",
      "Epoch 178/200\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.0351 - accuracy: 0.9962\n",
      "Epoch 179/200\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9962\n",
      "Epoch 180/200\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.0347 - accuracy: 0.9962\n",
      "Epoch 181/200\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.0345 - accuracy: 0.9962\n",
      "Epoch 182/200\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.0344 - accuracy: 0.9962\n",
      "Epoch 183/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0342 - accuracy: 0.9962\n",
      "Epoch 184/200\n",
      "33/33 [==============================] - 0s 740us/step - loss: 0.0340 - accuracy: 0.9962\n",
      "Epoch 185/200\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.0339 - accuracy: 0.9962\n",
      "Epoch 186/200\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.0337 - accuracy: 0.9962\n",
      "Epoch 187/200\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.0336 - accuracy: 0.9962\n",
      "Epoch 188/200\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.0334 - accuracy: 0.9962\n",
      "Epoch 189/200\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.0333 - accuracy: 0.9962\n",
      "Epoch 190/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0331 - accuracy: 0.9962\n",
      "Epoch 191/200\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0330 - accuracy: 0.9962\n",
      "Epoch 192/200\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.0329 - accuracy: 0.9962\n",
      "Epoch 193/200\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.0327 - accuracy: 0.9962\n",
      "Epoch 194/200\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.0326 - accuracy: 0.9962\n",
      "Epoch 195/200\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.0325 - accuracy: 0.9962\n",
      "Epoch 196/200\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.0323 - accuracy: 0.9962\n",
      "Epoch 197/200\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.0322 - accuracy: 0.9962\n",
      "Epoch 198/200\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.0321 - accuracy: 0.9962\n",
      "Epoch 199/200\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.0320 - accuracy: 0.9962\n",
      "Epoch 200/200\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.0319 - accuracy: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22057089b50>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the weights from the model using  `Model.get_layer` and `Layer.get_weights`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer(\"w2v_embedding\").get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save the vectors and metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open(\"./datasets/vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"./datasets/metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue   # skip 0, it's padding\n",
    "  vec = weights[index]\n",
    "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the `./datasets/vectors.tsv` and `./datasets/metadata.tsv` to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/)\n",
    "\n",
    "- Use PCA\n",
    "- Click \"Spherical Data\"\n",
    "- Contrast between two distanced points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
